# -*- coding: utf-8 -*-
"""AIML5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jEupHkzvwzKojrbebrkfE3Zbb3vOgIWj
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torchvision.utils import save_image, make_grid
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
from collections import Counter

# ==========================================
# 1. INPUT PARAMETERS & CONFIGURATION [cite: 29-36]
# ==========================================
# You can change these variables or use input() as requested
config = {
    'dataset_choice': 'fashion',      # Options: 'mnist' or 'fashion'
    'epochs': 30,                   # Recommended: 30-100
    'batch_size': 128,              # Recommended: 64 or 128
    'noise_dim': 100,               # Dimension of random noise vector
    'learning_rate': 0.0002,        # Adam optimizer learning rate
    'save_interval': 5,             # Save samples every k epochs
    'device': 'cuda' if torch.cuda.is_available() else 'cpu'
}

print(f"Running on device: {config['device']}")
print(f"Configuration: {config}")

# Create directories for outputs [cite: 55, 58]
os.makedirs('generated_samples', exist_ok=True)
os.makedirs('final_generated_images', exist_ok=True)

# ==========================================
# 2. DATASET LOADING [cite: 24-28]
# ==========================================
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,)) # Normalize to [-1, 1] [cite: 45]
])

if config['dataset_choice'] == 'mnist':
    dataset = torchvision.datasets.MNIST(root='./data', train=True,
                                       transform=transform, download=True)
elif config['dataset_choice'] == 'fashion':
    dataset = torchvision.datasets.FashionMNIST(root='./data', train=True,
                                              transform=transform, download=True)
else:
    raise ValueError("Invalid dataset_choice. Use 'mnist' or 'fashion'.")

dataloader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=True)

# ==========================================
# 3. PRE-TRAINED CLASSIFIER SETUP
# ==========================================
# To fulfill "Output 4", we need a classifier. Since we don't have an external file,
# we quickly train a simple CNN on the real data first to act as our "Pre-trained Model".
class SimpleClassifier(nn.Module):
    def __init__(self):
        super(SimpleClassifier, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = nn.functional.relu(x)
        x = self.conv2(x)
        x = nn.functional.relu(x)
        x = nn.functional.max_pool2d(x, 2)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = nn.functional.relu(x)
        x = self.fc2(x)
        return nn.functional.log_softmax(x, dim=1)

print("\n--- Phase 0: Setting up 'Pre-trained' Classifier ---")
classifier = SimpleClassifier().to(config['device'])
# We load a quick loop just to ensure it works (in a real scenario, you'd load weights)
cls_optimizer = optim.Adam(classifier.parameters(), lr=0.001)
cls_criterion = nn.CrossEntropyLoss()

# Train classifier for just 1 epoch to save time for the lab
classifier.train()
for batch_idx, (data, target) in enumerate(dataloader):
    data, target = data.to(config['device']), target.to(config['device'])
    cls_optimizer.zero_grad()
    output = classifier(data)
    loss = cls_criterion(output, target)
    loss.backward()
    cls_optimizer.step()
    if batch_idx > 100: break # Stop early, just proving concept
print("Classifier setup complete.\n")

# ==========================================
# 4. GAN ARCHITECTURE [cite: 39-40]
# ==========================================

# Generator: Noise (100) -> Image (1x28x28)
class Generator(nn.Module):
    def __init__(self, noise_dim):
        super(Generator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(noise_dim, 256),
            nn.LeakyReLU(0.2),
            nn.BatchNorm1d(256),
            nn.Linear(256, 512),
            nn.LeakyReLU(0.2),
            nn.BatchNorm1d(512),
            nn.Linear(512, 1024),
            nn.LeakyReLU(0.2),
            nn.BatchNorm1d(1024),
            nn.Linear(1024, 28*28),
            nn.Tanh() # Output -1 to 1 to match data normalization
        )

    def forward(self, x):
        img = self.model(x)
        img = img.view(img.size(0), 1, 28, 28) # Reshape to image dimensions [cite: 46]
        return img

# Discriminator: Image (1x28x28) -> Probability (0-1)
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(28*28, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 1),
            nn.Sigmoid() # Output probability
        )

    def forward(self, img):
        img_flat = img.view(img.size(0), -1) # Flatten
        validity = self.model(img_flat)
        return validity

# Initialize Models
generator = Generator(config['noise_dim']).to(config['device'])
discriminator = Discriminator().to(config['device'])

# Optimizers and Loss
optimizer_G = optim.Adam(generator.parameters(), lr=config['learning_rate'])
optimizer_D = optim.Adam(discriminator.parameters(), lr=config['learning_rate'])
adversarial_loss = nn.BCELoss() # Binary Cross Entropy

# ==========================================
# 5. TRAINING LOOP [cite: 41]
# ==========================================

print(f"--- Phase 1: Training GAN for {config['epochs']} epochs ---")

for epoch in range(config['epochs']):
    for i, (imgs, _) in enumerate(dataloader):

        # Ground truths
        real_imgs = imgs.to(config['device'])
        batch_size = real_imgs.size(0)

        # Labels: 1 for real, 0 for fake
        valid = torch.ones(batch_size, 1).to(config['device'])
        fake = torch.zeros(batch_size, 1).to(config['device'])

        # -----------------
        #  Train Generator
        # -----------------
        optimizer_G.zero_grad()

        # Sample noise
        z = torch.randn(batch_size, config['noise_dim']).to(config['device'])

        # Generate images
        gen_imgs = generator(z)

        # Loss measures generator's ability to fool the discriminator
        g_loss = adversarial_loss(discriminator(gen_imgs), valid)

        g_loss.backward()
        optimizer_G.step()

        # ---------------------
        #  Train Discriminator
        # ---------------------
        optimizer_D.zero_grad()

        # Measure discriminator's ability to classify real from generated samples
        real_loss = adversarial_loss(discriminator(real_imgs), valid)
        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)
        d_loss = (real_loss + fake_loss) / 2

        d_loss.backward()
        optimizer_D.step()

        # Calculate Discriminator Accuracy for logging
        with torch.no_grad():
            pred_real = discriminator(real_imgs) >= 0.5
            pred_fake = discriminator(gen_imgs.detach()) < 0.5
            d_acc = (pred_real.float().mean() + pred_fake.float().mean()) / 2 * 100

    # Output 1: Training Logs [cite: 51-53]
    print(f"Epoch {epoch+1}/{config['epochs']} | D_loss: {d_loss.item():.4f} | D_acc: {d_acc.item():.2f}% | G_loss: {g_loss.item():.4f}")

    # Output 2: Save Generated Samples Periodically [cite: 54-56]
    if (epoch + 1) % config['save_interval'] == 0:
        save_file = f"generated_samples/epoch_{epoch+1:02d}.png"
        save_image(gen_imgs.data[:25], save_file, nrow=5, normalize=True) # Grid 5x5 [cite: 48]
        print(f"   -> Saved samples to {save_file}")

# ==========================================
# 6. FINAL GENERATION & EVALUATION [cite: 57-60]
# ==========================================

print("\n--- Phase 2: Final Generation & Evaluation ---")

# A. Generate 100 synthetic images
num_final_samples = 100
z_final = torch.randn(num_final_samples, config['noise_dim']).to(config['device'])
final_imgs = generator(z_final)

# Save them individually as requested
for k in range(num_final_samples):
    save_image(final_imgs[k], f"final_generated_images/img_{k}.png", normalize=True)

print(f"Saved 100 final images to 'final_generated_images/' folder.")

# B. Predict Labels using Pre-trained Classifier
classifier.eval()
with torch.no_grad():
    outputs = classifier(final_imgs)
    _, predicted_labels = torch.max(outputs, 1)

# Map numeric labels to names for better readability
if config['dataset_choice'] == 'fashion':
    label_map = {0: 'T-shirt/top', 1: 'Trouser', 2: 'Pullover', 3: 'Dress', 4: 'Coat',
                 5: 'Sandal', 6: 'Shirt', 7: 'Sneaker', 8: 'Bag', 9: 'Ankle boot'}
else: # MNIST
    label_map = {i: str(i) for i in range(10)}

predicted_names = [label_map[p.item()] for p in predicted_labels]
label_counts = Counter(predicted_names)

print("\n--- Output 4: Label Distribution of Generated Images ---")
print("How the pre-trained classifier sees our fake images:")
for label, count in label_counts.items():
    print(f"{label}: {count}")

# Optional: Visualize a few final samples with their predicted labels
grid_img = make_grid(final_imgs[:10], nrow=10, normalize=True)
plt.figure(figsize=(15, 2))
plt.imshow(grid_img.permute(1, 2, 0).cpu().numpy())
plt.title(f"First 10 Generated Images | Preds: {[predicted_names[i] for i in range(10)]}")
plt.axis('off')
plt.show()

import os
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torchvision.utils import save_image, make_grid
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
from collections import Counter

# ==========================================
# 1. INPUT PARAMETERS & CONFIGURATION [cite: 29-36]
# ==========================================
# You can change these variables or use input() as requested
config = {
    'dataset_choice': 'fashion',      # Options: 'mnist' or 'fashion'
    'epochs': 30,                   # Recommended: 30-100
    'batch_size': 128,              # Recommended: 64 or 128
    'noise_dim': 100,               # Dimension of random noise vector
    'learning_rate': 0.0002,        # Adam optimizer learning rate
    'save_interval': 5,             # Save samples every k epochs
    'device': 'cuda' if torch.cuda.is_available() else 'cpu'
}

print(f"Running on device: {config['device']}")
print(f"Configuration: {config}")

# Create directories for outputs [cite: 55, 58]
os.makedirs('generated_samples', exist_ok=True)
os.makedirs('final_generated_images', exist_ok=True)

# ==========================================
# 2. DATASET LOADING [cite: 24-28]
# ==========================================
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,)) # Normalize to [-1, 1] [cite: 45]
])

if config['dataset_choice'] == 'mnist':
    dataset = torchvision.datasets.MNIST(root='./data', train=True,
                                       transform=transform, download=True)
elif config['dataset_choice'] == 'fashion':
    dataset = torchvision.datasets.FashionMNIST(root='./data', train=True,
                                              transform=transform, download=True)
else:
    raise ValueError("Invalid dataset_choice. Use 'mnist' or 'fashion'.")

dataloader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=True)

# ==========================================
# 3. PRE-TRAINED CLASSIFIER SETUP
# ==========================================
# To fulfill "Output 4", we need a classifier. Since we don't have an external file,
# we quickly train a simple CNN on the real data first to act as our "Pre-trained Model".
class SimpleClassifier(nn.Module):
    def __init__(self):
        super(SimpleClassifier, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = nn.functional.relu(x)
        x = self.conv2(x)
        x = nn.functional.relu(x)
        x = nn.functional.max_pool2d(x, 2)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = nn.functional.relu(x)
        x = self.fc2(x)
        return nn.functional.log_softmax(x, dim=1)

print("\n--- Phase 0: Setting up 'Pre-trained' Classifier ---")
classifier = SimpleClassifier().to(config['device'])
# We load a quick loop just to ensure it works (in a real scenario, you'd load weights)
cls_optimizer = optim.Adam(classifier.parameters(), lr=0.001)
cls_criterion = nn.CrossEntropyLoss()

# Train classifier for just 1 epoch to save time for the lab
classifier.train()
for batch_idx, (data, target) in enumerate(dataloader):
    data, target = data.to(config['device']), target.to(config['device'])
    cls_optimizer.zero_grad()
    output = classifier(data)
    loss = cls_criterion(output, target)
    loss.backward()
    cls_optimizer.step()
    if batch_idx > 100: break # Stop early, just proving concept
print("Classifier setup complete.\n")

# ==========================================
# 4. GAN ARCHITECTURE [cite: 39-40]
# ==========================================

# Generator: Noise (100) -> Image (1x28x28)
class Generator(nn.Module):
    def __init__(self, noise_dim):
        super(Generator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(noise_dim, 256),
            nn.LeakyReLU(0.2),
            nn.BatchNorm1d(256),
            nn.Linear(256, 512),
            nn.LeakyReLU(0.2),
            nn.BatchNorm1d(512),
            nn.Linear(512, 1024),
            nn.LeakyReLU(0.2),
            nn.BatchNorm1d(1024),
            nn.Linear(1024, 28*28),
            nn.Tanh() # Output -1 to 1 to match data normalization
        )

    def forward(self, x):
        img = self.model(x)
        img = img.view(img.size(0), 1, 28, 28) # Reshape to image dimensions [cite: 46]
        return img

# Discriminator: Image (1x28x28) -> Probability (0-1)
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(28*28, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 1),
            nn.Sigmoid() # Output probability
        )

    def forward(self, img):
        img_flat = img.view(img.size(0), -1) # Flatten
        validity = self.model(img_flat)
        return validity

# Initialize Models
generator = Generator(config['noise_dim']).to(config['device'])
discriminator = Discriminator().to(config['device'])

# Optimizers and Loss
optimizer_G = optim.Adam(generator.parameters(), lr=config['learning_rate'])
optimizer_D = optim.Adam(discriminator.parameters(), lr=config['learning_rate'])
adversarial_loss = nn.BCELoss() # Binary Cross Entropy

# ==========================================
# 5. TRAINING LOOP [cite: 41]
# ==========================================

print(f"--- Phase 1: Training GAN for {config['epochs']} epochs ---")

for epoch in range(config['epochs']):
    for i, (imgs, _) in enumerate(dataloader):

        # Ground truths
        real_imgs = imgs.to(config['device'])
        batch_size = real_imgs.size(0)

        # Labels: 1 for real, 0 for fake
        valid = torch.ones(batch_size, 1).to(config['device'])
        fake = torch.zeros(batch_size, 1).to(config['device'])

        # -----------------
        #  Train Generator
        # -----------------
        optimizer_G.zero_grad()

        # Sample noise
        z = torch.randn(batch_size, config['noise_dim']).to(config['device'])

        # Generate images
        gen_imgs = generator(z)

        # Loss measures generator's ability to fool the discriminator
        g_loss = adversarial_loss(discriminator(gen_imgs), valid)

        g_loss.backward()
        optimizer_G.step()

        # ---------------------
        #  Train Discriminator
        # ---------------------
        optimizer_D.zero_grad()

        # Measure discriminator's ability to classify real from generated samples
        real_loss = adversarial_loss(discriminator(real_imgs), valid)
        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)
        d_loss = (real_loss + fake_loss) / 2

        d_loss.backward()
        optimizer_D.step()

        # Calculate Discriminator Accuracy for logging
        with torch.no_grad():
            pred_real = discriminator(real_imgs) >= 0.5
            pred_fake = discriminator(gen_imgs.detach()) < 0.5
            d_acc = (pred_real.float().mean() + pred_fake.float().mean()) / 2 * 100

    # Output 1: Training Logs [cite: 51-53]
    print(f"Epoch {epoch+1}/{config['epochs']} | D_loss: {d_loss.item():.4f} | D_acc: {d_acc.item():.2f}% | G_loss: {g_loss.item():.4f}")

    # Output 2: Save Generated Samples Periodically [cite: 54-56]
    if (epoch + 1) % config['save_interval'] == 0:
        save_file = f"generated_samples/epoch_{epoch+1:02d}.png"
        save_image(gen_imgs.data[:25], save_file, nrow=5, normalize=True) # Grid 5x5 [cite: 48]
        print(f"   -> Saved samples to {save_file}")

# ==========================================
# 6. FINAL GENERATION & EVALUATION [cite: 57-60]
# ==========================================

print("\n--- Phase 2: Final Generation & Evaluation ---")

# A. Generate 100 synthetic images
num_final_samples = 100
z_final = torch.randn(num_final_samples, config['noise_dim']).to(config['device'])
final_imgs = generator(z_final)

# Save them individually as requested
for k in range(num_final_samples):
    save_image(final_imgs[k], f"final_generated_images/img_{k}.png", normalize=True)

print(f"Saved 100 final images to 'final_generated_images/' folder.")

# B. Predict Labels using Pre-trained Classifier
classifier.eval()
with torch.no_grad():
    outputs = classifier(final_imgs)
    _, predicted_labels = torch.max(outputs, 1)

# Map numeric labels to names for better readability
if config['dataset_choice'] == 'fashion':
    label_map = {0: 'T-shirt/top', 1: 'Trouser', 2: 'Pullover', 3: 'Dress', 4: 'Coat',
                 5: 'Sandal', 6: 'Shirt', 7: 'Sneaker', 8: 'Bag', 9: 'Ankle boot'}
else: # MNIST
    label_map = {i: str(i) for i in range(10)}

predicted_names = [label_map[p.item()] for p in predicted_labels]
label_counts = Counter(predicted_names)

print("\n--- Output 4: Label Distribution of Generated Images ---")
print("How the pre-trained classifier sees our fake images:")
for label, count in label_counts.items():
    print(f"{label}: {count}")

# Optional: Visualize a few final samples with their predicted labels
grid_img = make_grid(final_imgs[:10], nrow=10, normalize=True)
plt.figure(figsize=(15, 2))
plt.imshow(grid_img.permute(1, 2, 0).cpu().numpy())
plt.title(f"First 10 Generated Images | Preds: {[predicted_names[i] for i in range(10)]}")
plt.axis('off')
plt.show()

import os
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torchvision.utils import save_image, make_grid
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
from collections import Counter

# ==========================================
# 1. CONFIGURATION (MNIST)
# ==========================================
config = {
    'dataset_choice': 'mnist',
    'epochs': 30,
    'batch_size': 128,
    'noise_dim': 100,
    'learning_rate': 0.0002,
    'save_interval': 5,
    'device': 'cuda' if torch.cuda.is_available() else 'cpu'
}

print(f"Running on device: {config['device']}")

# Output folders (MNIST specific)
os.makedirs('mnist_generated_samples', exist_ok=True)
os.makedirs('mnist_final_images', exist_ok=True)

# ==========================================
# 2. MNIST DATASET LOADING
# ==========================================
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

dataset = torchvision.datasets.MNIST(
    root='./data',
    train=True,
    transform=transform,
    download=True
)

dataloader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=True)

# ==========================================
# 3. SIMPLE CLASSIFIER (FOR EVALUATION)
# ==========================================
class SimpleClassifier(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = torch.relu(self.conv2(x))
        x = torch.max_pool2d(x, 2)
        x = torch.flatten(x, 1)
        x = torch.relu(self.fc1(x))
        return self.fc2(x)

classifier = SimpleClassifier().to(config['device'])
cls_optimizer = optim.Adam(classifier.parameters(), lr=0.001)
cls_loss = nn.CrossEntropyLoss()

# Quick training (1 epoch)
classifier.train()
for i, (data, target) in enumerate(dataloader):
    data, target = data.to(config['device']), target.to(config['device'])
    cls_optimizer.zero_grad()
    output = classifier(data)
    loss = cls_loss(output, target)
    loss.backward()
    cls_optimizer.step()
    if i > 100:
        break

# ==========================================
# 4. GAN MODELS
# ==========================================
class Generator(nn.Module):
    def __init__(self, noise_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(noise_dim, 256),
            nn.LeakyReLU(0.2),
            nn.BatchNorm1d(256),
            nn.Linear(256, 512),
            nn.LeakyReLU(0.2),
            nn.BatchNorm1d(512),
            nn.Linear(512, 1024),
            nn.LeakyReLU(0.2),
            nn.BatchNorm1d(1024),
            nn.Linear(1024, 28 * 28),
            nn.Tanh()
        )

    def forward(self, z):
        img = self.net(z)
        return img.view(img.size(0), 1, 28, 28)


class Discriminator(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(28 * 28, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 1),
            nn.Sigmoid()
        )

    def forward(self, img):
        img = img.view(img.size(0), -1)
        return self.net(img)


generator = Generator(config['noise_dim']).to(config['device'])
discriminator = Discriminator().to(config['device'])

optimizer_G = optim.Adam(generator.parameters(), lr=config['learning_rate'])
optimizer_D = optim.Adam(discriminator.parameters(), lr=config['learning_rate'])
criterion = nn.BCELoss()

# ==========================================
# 5. GAN TRAINING
# ==========================================
for epoch in range(config['epochs']):
    for imgs, _ in dataloader:
        imgs = imgs.to(config['device'])
        batch_size = imgs.size(0)

        valid = torch.ones(batch_size, 1).to(config['device'])
        fake = torch.zeros(batch_size, 1).to(config['device'])

        # Train Generator
        optimizer_G.zero_grad()
        z = torch.randn(batch_size, config['noise_dim']).to(config['device'])
        gen_imgs = generator(z)
        g_loss = criterion(discriminator(gen_imgs), valid)
        g_loss.backward()
        optimizer_G.step()

        # Train Discriminator
        optimizer_D.zero_grad()
        real_loss = criterion(discriminator(imgs), valid)
        fake_loss = criterion(discriminator(gen_imgs.detach()), fake)
        d_loss = (real_loss + fake_loss) / 2
        d_loss.backward()
        optimizer_D.step()

    print(f"Epoch [{epoch+1}/{config['epochs']}]  D_loss: {d_loss:.4f}  G_loss: {g_loss:.4f}")

    if (epoch + 1) % config['save_interval'] == 0:
        save_image(
            gen_imgs[:25],
            f"mnist_generated_samples/mnist_epoch_{epoch+1}.png",
            nrow=5,
            normalize=True
        )

# ==========================================
# 6. FINAL MNIST IMAGE GENERATION
# ==========================================
z = torch.randn(100, config['noise_dim']).to(config['device'])
final_imgs = generator(z)

for i in range(100):
    save_image(
        final_imgs[i],
        f"mnist_final_images/mnist_img_{i}.png",
        normalize=True
    )

# ==========================================
# 7. CLASSIFIER LABEL DISTRIBUTION
# ==========================================
classifier.eval()
with torch.no_grad():
    preds = classifier(final_imgs).argmax(dim=1)

counts = Counter(preds.cpu().numpy())
print("\nLabel distribution of generated MNIST digits:")
for digit, count in sorted(counts.items()):
    print(f"Digit {digit}: {count}")